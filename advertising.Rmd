```{r}
# Defining the question
#### Identifying the individuals who are most likely going to click on the Kenyan entrepreneur's ads

# Metric for success
#### supervised learning

# Context
#### a Kenyan entreneur has started an online cryptography course and would want to advertise it on her blog. she targets audiences from various countries.

# Experimental design 
#### kNN
#### SVM
#### Naive Bayes

# Appropriateness of the data
#### The Kenyan Entrepreneur has already collected the data that will enable me to work on. 
```


```{r}
# Loading the dataset
advertising <- read.csv("C:/Users/I/Downloads/advertising.csv")

```

# Checking the dataset
```{r}
# checking for the data types in the dataset
str(advertising)
```


```{r}
# Previwing the top of the dataset
head(advertising)
```


```{r}
# Previewing the bottom of the dataset
tail(advertising)
```


```{r}
# Checking the columns of the dataset
colnames(advertising)
```



```{r}
# Checking for the data types in the dataset
dim(advertising)
```

there are 1000 rows and 10 columns



# Tidying the dataset
```{r}
# Checking for missing data
colSums(is.na(advertising))
```

there are no missing data in the dataset


```{r}
# Showing whether there are repeat entries
advertising[duplicated(advertising), ]
```


there are no duplicated data in the dataset
# Checking for outliers
```{r}
# In Daily.Time.Spent.on.Site
boxplot(advertising$Daily.Time.Spent.on.Site)
```


there are no outliers in the time spent on site columns
```{r}
# In Age  
boxplot(advertising$Age)
```

there are no outliers in the Age column
```{r}
# In Area.Income
boxplot(advertising$Area.Income)
```

there are outliers that are less than 20,000. The outliers look valid since they are from a dataset that has been collected by the entrepreneur over a period of time
 
```{r}
# Male
boxplot(advertising$Male)
```



there are no outliers in the column
```{r}
# Clicked.on.Ad
boxplot(advertising$Clicked.on.Ad)
```

there are no clicks on ad

# Feature engineering
```{r}
# Making the column names uniform
names(advertising)[names(advertising)=="Daily.Time.Spent.on.Site"] <- "daily_time_spent_on_site"
names(advertising)[names(advertising)=="Age"] <- "age"
names(advertising)[names(advertising)=="Area.Income"] <- "area_income"
names(advertising)[names(advertising)=="Daily.Internet.Usage"] <- "daily_internet_usage"
names(advertising)[names(advertising)=="Ad.Topic.Line"] <- "ad_topic_line"
names(advertising)[names(advertising)=="City"] <- "city"
names(advertising)[names(advertising)=="Male"] <- "sex" #changing the male name to sex because is representing males and female
names(advertising)[names(advertising)=="Country"] <- "country"
names(advertising)[names(advertising)=="Timestamp"] <- "timestamp"
names(advertising)[names(advertising)=="Clicked.on.Ad"] <- "clicked_on_ad"
```

```{r}
colnames(advertising)
```

verifying that the suggested changes has taken place

```{r}
# Removing the ad_topic_line column because i do not see its relevancy
advertising$ad_topic_line <- NULL
```

```{r}
colnames(advertising)
```
we have now verified that the ad topic line has been removed from the dataset

# Univariate data analysis
```{r}
# Daily time spent on site
x = hist(advertising$daily_time_spent_on_site,
         main = "Daily time Spent on Site",
         xlab = "Daily time Spent on Site",
         col ='green')
```

the data is skewed to the left and most of the data is distributed between 65 and 85       

```{r}
# Age
x = hist(advertising$age,
         main = "Age",
         xlab = "Age",
         col ='green')
```
data is skewed to the right and most of the data is distributed between 25 and 40
```{r}
# Area_income
x = hist(advertising$area_income,
         main = "area income",
         xlab = "area income",
         col ='green')
```
data is skewed to the left and most of the data is distributed between 50,000 and 70,000
```{r}
# Daily_internet_usage
x = hist(advertising$daily_internet_usage,
         main = "daily internet usage",
         xlab = "daily internet usage",
         col ='green')
```
 
data is used a lot and most of it is distributed between 200 and 225
   
```{r}
# Sex
table(advertising$sex) # there are 519 males and 481 females

barplot(table(advertising$sex),
    main="sex")
```
```{r}
# Country
table(advertising$country)
sample <- head(table(advertising$country))
barplot(sample, main="Country")
```

we see that afghanistan has the most appearance.
  
```{r}
# Clicked_on_ad

pie(table(advertising$clicked_on_ad))
```

there are 500 no clicks and 500 click on ads


# Bivariate Analysis

```{r}
# Converting variables into string so as to enable to make good visualisations.
a = c('city', 'country')
for (i in a) {
  advertising[,i] = as.factor(advertising[,i])
}
```


```{r}
# Converting variables into factors so as to enable to make good visualisations.
b = c('sex', 'clicked_on_ad')
for (i in b) {
advertising[,i] = as.factor(advertising[,i])
}
```



```{r}
# Importing the ggplot library
library(ggplot2)

```


```{r}
# Sex vs clicked on ad 
sex_vs_ad = ggplot(data = advertising, aes(x = sex, fill = clicked_on_ad))+
geom_bar(width = 0.5)
sex_vs_ad

```


```{r}
# Area_income vs clicked on ad
area_vs_ad = ggplot(data = advertising, aes(x = area_income, fill = clicked_on_ad))+ 
geom_histogram(bins = 25)
area_vs_ad
```
many people who had an income between 50,000 and 70,000 did not click on the ad compared to those who did

```{r}
# Daily_internet_usage vs clicked on ad
daily_vs_ad = ggplot(data = advertising, aes(x = daily_internet_usage, fill = clicked_on_ad))+ 
geom_histogram(bins = 25)
daily_vs_ad
```
between 100 and 150 there are more people who clicked the ad than those who did not.

```{r}
# Age vs clicked on ad
age_vs_ad = ggplot(data = advertising, aes(x = age, fill = clicked_on_ad))+ 
geom_histogram(bins = 25)
age_vs_ad
```
generally many people do not click ads
 
# Recommendation
#### The entrepreneur should focus on people with an income between 10,000 and 50,000
#### The entrepreneur should focus on people with the age between 45 and 60
#### The entrepreneur should focus on people with a daily internet usage between 100 and 175

# Modelling
#### KNN
```{r}
# Creating a random number equal 90% of total number of rows
rand <- sample(1:nrow(advertising),0.9 * nrow(advertising))
```


```{r}
# The normalization function is created
normal <-function(x) { (x -min(x))/(max(x)-min(x))   }
 
# Normalization function is applied to the dataframe
advert <- as.data.frame(lapply(advertising[,c(1,2,3,4)], normal))
```

```{r}
# The training dataset extracted
advert_train <- advert[rand,]
```

```{r}
# The test dataset extracted
advert_test <- advert[-rand,]
```

```{r}
# getting the training and test data from the target
advert_target <- advertising[rand,9]
test_advert <- advertising[-rand,9]
```

```{r}
# Running the knn function
library(class)
pr <- knn(advert_train,advert_test,cl=advert_target,k=20)
```

```{r}
# Creating the confucion matrix
tb <- table(pr,test_advert)
tb
```
there are 49 and 50 correctly predicted items
```{r}
# Checking the accuracy
accuracy <- function(x){sum(diag(x)/(sum(rowSums(x)))) * 100}
accuracy(tb)
```
the accuracy score is 99% which is very good


# SVM
```{r}
# We will now install and load the required packages
# ---
#  
install.packages('tidyverse')
library(tidyverse)

install.packages('ggplot2')
library(ggplot2)

install.packages('caret')
library(caret)

install.packages('caretEnsemble')
library(caretEnsemble)

install.packages('psych')
library(psych)

install.packages('Amelia')
library(Amelia)

install.packages('mice')
library(mice)

install.packages('GGally')
library(GGally)

install.packages('rpart')
library(rpart)

install.packages('randomForest')
library(randomForest)
```

```{r}
colnames(advertising)
```

```{r}
# splitting the dataset into train and test
intrain <- createDataPartition(y = advertising$clicked_on_ad, p= 0.7, list = FALSE)
training <- advertising[intrain,]
testing <- advertising[-intrain,]
```

```{r}
# We check the dimensions of out training dataframe and testing dataframe
# ---
# 
dim(training); 
dim(testing);
```
```{r}
testing
```

```{r}
trctrl <- trainControl(method = "repeatedcv", number = 10, repeats = 3)

svm_Linear <- train(clicked_on_ad ~., data = training, method = "svmLinear",
trControl=trctrl,
preProcess = c("center", "scale"),
tuneLength = 10)
```

```{r}
# We can then check the reult of our train() model as shown below
# ---
# 
svm_Linear
```

```{r}
# We can use the predict() method for predicting results as shown below. 
# We pass 2 arguements, our trained model and our testing data frame.
# ---
# 
test_pred <- predict(svm_Linear, newdata = testing)
test_pred
```

```{r}
# Now checking for our accuracy of our model by using a confusion matrix 
# ---
# 
confusionMatrix(table(test_pred, testing$clicked_on_ad))
```
144 and 133 items were correctly predicted
the accuracy is 92.33% 
# Naive Bayes



```{r}
# Splitting data into training and test data sets
# ---
# 
indxTrain <- createDataPartition(y = advertising$clicked_on_ad,p = 0.75,list = FALSE)
training <- advertising[indxTrain,]
testing <- advertising[-indxTrain,]
```

```{r}
x = training[,-9]
y = training$clicked_on_ad
```

```{r}
# Loading our inbuilt e1071 package that holds the Naive Bayes function.
# ---
# 
library(e1071)
```

```{r}
# Now building our model 
# ---
# 
model = train(x,y,'nb',trControl=trainControl(method='cv',number=10))
```

```{r}
# Model Evalution
# ---
# Predicting our testing set
# 
Predict <- predict(model,newdata = testing )
```

```{r}
# Getting the confusion matrix to see accuracy value and other parameter values
# ---
# 
confusionMatrix(Predict, testing$clicked_on_ad )
```
there are 120 and 121 corectly predicted click on ads
the accuracy score is 96.4%

# Challenging the model
KNN is the best model of the three . it has an accuracy of 99%.
there is no point of challenging it for its the best

# Recommendation
all is well. we can work with the KNN model.